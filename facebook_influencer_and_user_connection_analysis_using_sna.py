# -*- coding: utf-8 -*-
"""Facebook_Influencer_and_User_Connection_Analysis_using_SNA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rs3P9pB8hvOYmOi2enp5_XWNV7yqUEal

# Facebook Influencer and User Connection Analysis using SNA
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install 'scipy>=1.8'

# Commented out IPython magic to ensure Python compatibility.
# %pip install 'networkx<2.7'

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy.stats import norm
from sklearn.preprocessing import StandardScaler
from scipy import stats
from statistics import mean

import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

matrix = pd.read_csv('facebook_edges.csv',sep=",")
#matrix = pd.read_csv('facebook_combined.txt',sep=" ",header=None)

"""### First insights about the data"""

matrix.head()

matrix.shape

matrix.describe()

numpy_matrix = matrix.values

numpy_matrix

import networkx as nx

# Instantiate the graph
G1 = nx.Graph()
# add node/edge pairs
G1.add_edges_from(numpy_matrix)

G1.number_of_nodes()

G1.number_of_edges()

"""### 1) Degree Centrality

#### Assumption: important nodes have many connections.
#### The most basic measure of centrality: number of neighbors.
"""

def degree_centrality(G1):
    #Code to figure out the person who has the biggest number of neighbors
    maximum = 0
    node = 0
    degree_centrality = nx.degree_centrality(G1)
    for i in G1.nodes() :
        if (degree_centrality[i]> maximum)  :
            maximum = degree_centrality[i]
            node = i
    #print(node)
    return node

def top_percent_degree_centrality(G1 , per = 0.1):
    degree_centrality = nx.degree_centrality(G1)

    lis=[]
    dic = dict(degree_centrality)

    #Top per of the data
    n= int(per * G1.number_of_nodes())

    for i in range (0,n) :
        maximum = 0
        node = 0
        for elt in dic :
            if dic[elt]> maximum  :
                maximum = dic[elt]
                node = elt

        del dic[node]
        lis.append(node)
    lis_degree=lis
    #print(lis_degree)
    return lis_degree

"""### 2) Closeness centrality

#### Assumption: important nodes are close to other nodes.
"""

def closeness_centrality(G1):
    maximum = 0
    node = 0
    closeness_centrality = nx.closeness_centrality(G1)
    for i in G1.nodes() :
        if closeness_centrality[i]> maximum  :
            maximum = closeness_centrality[i]
            node = i
    #print(node)
    return node

def top_percent_closeness_centrality(G1 , per = 0.02 ):
    closeness_centrality = nx.closeness_centrality(G1)

    lis=[]
    dic = dict(closeness_centrality)

    #Top percent of the data
    n= int(per * G1.number_of_nodes())

    for i in range (0,n) :
        maximum = 0
        node = 0
        for elt in dic :
            if dic[elt]> maximum  :
                maximum = dic[elt]
                node = elt

        del dic[node]
        lis.append(node)
    lis_closeness=lis
    #print(lis_closeness)
    return lis_closeness

"""### 3) Betweenness Centrality

#### Assumption: important nodes connect other nodes.
"""

def betweenness_centrality(G1):
    maximum = 0
    node = 0
    betweenness_centrality = nx.betweenness_centrality(G1)
    for i in G1.nodes() :
        if betweenness_centrality[i]> maximum  :
            maximum = betweenness_centrality[i]
            node = i
    #print(node)
    return node

def top_percent_betweenness_centrality(G1 , per = 0.02):
    betweenness_centrality = nx.betweenness_centrality(G1)

    lis=[]
    dic = dict(betweenness_centrality)

    #Top per of the data
    n= int(per * G1.number_of_nodes())

    for i in range (0,n) :
        maximum = 0
        node = 0
        for elt in dic :
            if dic[elt]> maximum  :
                maximum = dic[elt]
                node = elt

        del dic[node]
        lis.append(node)
    lis_betweenness=lis
    #print(lis_betweenness)
    return lis_betweenness

"""### 4) Functions about influency"""

def polyvalent_influence (list1,list2,list3) :
    # Considering a person is a polyvalent influencer if he is among the top percent in the three classifications

    return [x for x in list1 if x in list2 and x in list3]

def list_of_components (components) :
    # list of lists where each list contain nodes of a single component

    list_of_components=[]
    count = 0
    for com in components :
        list_nodes=list(com)
        list_of_components.append(list_nodes)

    return list_of_components

# function to calculate the avg of the three parameters of centrality for the k th community

def extracting_graph_community (k,list_of_communities,numpy_matrix):
    sample_array = np.array(numpy_matrix)
    print(sample_array)
    list_edges_community_k = []
    for i in range(sample_array.shape[0]):
      print(i)
      if sample_array[i][0] in list_of_communities[k] :
          print("if1")
          if sample_array[i][1] in list_of_communities[k] :
              print("if2")
              list_edges_community_k.append([sample_array[i][0],sample_array[i][1]])

    # creating an array of the k th community
    sample_array=np.array(list_edges_community_k)
    print(sample_array)
    # creating the G_sample graph of the k th community

    # Instantiate the graph
    G_sample = nx.Graph()
    # add node/edge pairs
    G_sample.add_edges_from(sample_array)

    degree_centrality = nx.degree_centrality(G_sample)
    closeness_centralirty = nx.closeness_centrality(G_sample)
    betweenness_centrality = nx.betweenness_centrality(G_sample)

    avg_degree = mean(degree_centrality.values())
    avg_closeness = mean(closeness_centralirty.values())
    avg_betweenness = mean(betweenness_centrality.values())


    radius = nx.radius(G_sample)
    diameter = nx.diameter(G_sample)
    avg_shortest_path = nx.average_shortest_path_length(G_sample)



    smallest_bridge_number = nx.node_connectivity(G_sample)

    return (avg_degree,avg_closeness,avg_betweenness,radius,diameter,smallest_bridge_number,avg_shortest_path)

"""### Graph functions"""

def draw_graph (G_sample):
    # draw the network G1
    fig= plt.figure(figsize=(15,10))
    nx.draw_networkx(G_sample,with_labels=False,node_size=20,node_color='b')
    return

def color_list_of_nodes_in_graph (list1 , list2 , G ,node_size=40) :
    # given a list of nodes it colors the list of nodes in blue and the rest in green
    color_map = []
    for node in G.nodes() :
        if node in list1 :
            color_map.append('red')
        elif node in list2 :
            color_map.append('green')
        else: color_map.append('blue')
    fig= plt.figure(figsize=(18,14))
    nx.draw(G,node_color = color_map,with_labels=False,node_size=40,edge_color='Gray',alpha=0.9)
    plt.show()

def color_list_of_nodes_in_graph_centrality (list1 , list2 , G , centrality) :
    # given a list of nodes it colors the list of nodes in blue and the rest in green
    color_map = []
    size_map = []
    for node in G.nodes() :
        if node in list1 :
            color_map.append('red')
        elif node in list2 :
            color_map.append('green')
        else: color_map.append('blue')

        size_map.append(int(exp(centrality[node]+1)*100))

    fig= plt.figure(figsize=(18,14))
    nx.draw(G,node_color = color_map,with_labels=False,node_size=size_map,edge_color='Gray',alpha=0.9)
    plt.show()

def color_nodes (list) :
    # given a list of nodes it colors the list of nodes in blue and the rest in green
    color_map = []
    for node in G_sample :
        if node in list :
            color_map.append('red')
        else: color_map.append('blue')
    fig= plt.figure(figsize=(18,14))
    nx.draw(G_sample,node_color = color_map,with_labels=False,node_size=18,edge_color='Gray',alpha=0.8)
    plt.show()

def color_list_of_nodes_in_graph_centrality_weight (list1 , list2 , G , centrality) :
    # given a list of nodes it colors the list of nodes in blue and the rest in green
    color_map = []
    size_map = []

    for node in G.nodes() :
        if node in list1 :
            color_map.append('red')
        elif node in list2 :
            color_map.append('green')
        else: color_map.append('blue')

        size_map.append(int(exp(centrality[node]+1)*1000))

    edges , weights = zip(*nx.get_edge_attributes(G,'weight').items())

    listing = []
    for weight in weights :
        listing.append(exp(weight+1)*1000)
    weight = tuple (listing)

    fig= plt.figure(figsize=(18,14))
    nx.draw(G,node_color = color_map,font_color ='white',edgelist=edges,edge_color=weight ,edge_cmap = plt.cm.Blues ,with_labels=True,node_size=size_map,alpha=0.9)
    plt.show()

"""### Extracting Components from the Data

#### Rather than looking for influencers in all the graph we decide to focus on influencers on each component
"""

nx.number_connected_components(G1)

components= sorted(nx.connected_components(G1), key=len, reverse=True)

list_of_components = list_of_components (components)

len(list_of_components[0])

#drawing
size = float(nx.number_connected_components(G1))
pos = nx.spring_layout(G1)
count = 0.
fig= plt.figure(figsize=(18,14))

for com in components :
    count = count + 1.
    list_nodes = list(com)
    nx.draw_networkx_nodes(G1, pos, list_nodes, node_size = 40,edgecolors='black',node_color = str((count) / size),alpha=0.5)

nx.draw_networkx_edges(G1, pos, alpha=0.5)
plt.show()

"""### Details about communities"""

list_of_degrees = []
list_of_closeness = []
list_of_betweenness = []

list_of_radius = []
list_of_diameter = []


list_of_number_of_bridges = []
list_of_smallest_bridge_number = []

list_of_avg_shortest_path = []

for k in range(0,len(list_of_components)) :
    tuple = extracting_graph_community (k,list_of_components,numpy_matrix)
    print(k)

    list_of_degrees.append(tuple[0])
    list_of_closeness.append(tuple[1])
    list_of_betweenness.append(tuple[2])

    list_of_radius.append(tuple[3])
    list_of_diameter.append(tuple[4])

    list_of_number_of_bridges.append(tuple[5])

    list_of_avg_shortest_path.append(tuple[6])

len(list_of_degrees)

len(list_of_diameter)

len(list_of_number_of_bridges)

# Size of each community
list_of_sizes = [len(com) for com in list_of_components]

list_of_ids = [i for i in range (0,len(list_of_number_of_bridges))]

len(list_of_ids)

dic = {'Community_Id' :list_of_ids , 'Community_Size' : list_of_sizes , 'Avg_Deg_Cent' : list_of_degrees , 'Avg_Clo_Cent' : list_of_closeness , 'Avg_Bet_Cent' : list_of_betweenness ,'Radius':list_of_radius,'Diameter': list_of_diameter, 'Number_Bridges' : list_of_number_of_bridges,'Avg_shortest_path':list_of_avg_shortest_path}

community_details = pd.DataFrame(dic)

community_details

"""## A look on the biggest community on the Network

### Turning to focusing on single community
"""

sample_array=np.array(numpy_matrix)

# number of community
k = 0

# number of nodes in the k th community
len(list_of_components[k])

list_eges_community_k = []
for i in range(sample_array.shape[0]):
    if sample_array[i][0] in list_of_components[k] :
        if sample_array[i][1] in list_of_components[k] :
            list_eges_community_k.append([sample_array[i][0],sample_array[i][1]])

# creating an array of the k th community
sample_array=np.array(list_eges_community_k)

# number of edges in the community
sample_array.shape

# creating the G_sample graph of the k th community

# Instantiate the graph
G_sample = nx.Graph()
# add node/edge pairs
G_sample.add_edges_from(sample_array)

"""##### Finding the top 10 percent nodes in the centrality parameter classification

#### Degree centrality
"""

List_degree_centrality = top_percent_degree_centrality(G_sample, per = 0.1)

degree_centrality = nx.degree_centrality(G_sample)

# best degree centrality
degree_centrality[List_degree_centrality[0]]

color_nodes(List_degree_centrality)

"""#### Closeness centrality"""

List_closeness_centrality = top_percent_closeness_centrality(G_sample , per = 0.1)

color_nodes(List_closeness_centrality)

"""#### Betweenness centrality"""

List_betweenness_centrality = top_percent_betweenness_centrality(G_sample , per = 0.1)

color_nodes(List_betweenness_centrality)

"""#### Most Influential Nodes in all Centrality"""

List_centrality = [x for x in List_degree_centrality if x in List_closeness_centrality and x in List_betweenness_centrality]

List_centrality

color_nodes(List_centrality)

"""#### Bridges"""

# return a list of all node bridges
bridges=[]
max = 0
for elt in sorted(nx.bridges(G_sample)) :
        if elt[0] > max :
            max = elt[0]
            bridges.append(max)

color_nodes(bridges)

"""#### Bridges and influencers"""

color_list_of_nodes_in_graph (List_centrality,bridges, G_sample ,node_size=100)